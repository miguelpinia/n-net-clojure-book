\begin{savequote}[72mm]
  Machine consciousness refers to attempts by those who design and
  analyse informational machines to apply their methods to various
  ways of understanding consciousness and to examine the possible role
  of consciousness in informational machines.

  \qauthor{Igor Aleksander}
\end{savequote}

\chapter{Redes Neuronales}
\label{cha:redes}

En este capítulo estudiaremos los conceptos principales de varias
redes neuronales diviéndolas en dos tipos: Redes neuronales
supervisadas y redes neuronales no supervisadas. Esta distinción está
dada por la forma en que se efectúa el aprendizaje dentro de las redes
neuronales.\\

En el capítulo \ref{cha:Introducción} hablamos brevemente de estos
tipos de aprendizaje, donde mencionamos que el \textbf{aprendizaje
supervisado} es en el que se presenta ejemplos y salidas deseadas
para entrenar a la red y el \textbf{aprendizaje no supervisado} es el
que de forma autónoma, la red va descubriendo características acerca
de los datos que le son presentados.\\

A continuación explicaremos a detalle estos tipos de aprendizaje y
mostraremos las redes neuronales que están clasificadas bajo estos
tipos.

\section{Redes Neuronales Supervisadas}


\subsection{Perceptrón simple}

El perceptrón simple fue introducido por Frank Rossenblatt a finales
de los años 50, modelándolo como un dispositivo que mantuviera los
principios biológicos de una neurona y su habilidad para aprender. El
modelo implementado por el perceptrón simple es unidireccional,
compuesto por dos capas de neuronas, una sensorial o de entrada y otra
de respuesta o salida. La operación básica de esta red se puede
expresar a través de la función:

\begin{equation}
  \label{eq:perceptron1}
y_i(t) = f(\sum_{j=1}^nw_{ij}x_j-\theta_i), \forall i, 1 \leq i \leq n
\end{equation}

Las neuronas de entrada de este modelo no realizan ningún tipo de
procesamiento, solo se dedican a enviar información (consideraremos en
este momento solamente señales discretas {0, 1}) a las neuronas de
salida. Estas últimas utilizan como función de activación la función
escalón o \textbf{Función de Heaviside}. De este modo, la función
\ref{eq:perceptron1} se puede escribir de la siguiente forma:

\begin{equation}
  y_i = H(\sum_{j=1}^nw_{ij}x_j-\theta_i), \forall i, 1 \leq i \leq n
\end{equation}

La forma básica de uso del perceptrón es como clasificador, como para
realizar representaciones de funciones booleanas, esto último debido a
que su salida es de carácter discreto.\\

A continuación mostraremos un ejemplo de uso del perceptrón como
clasificador. En este ejemplo cada neurona del perceptrón representa
una clase, al que dado un vector de entrada, cierta neurona va a
responder con una salida ${0,1}$, donde 0 dice si el valor no
corresponde con el valor que representa y 1 si sí corresponde. Es
fácil observar que el perceptrón solo va a permitir clasificar cuando
las clases son linealmente separables, esto es, existe una única
condición de separación lineal o hiperplano.\\

Sea $n_i$ una neurona de nuestro ejemplo que sólo va a tener dos
entradas $x_1$ y $x_2$, con salida $y_i$ y cuya operación vamos a
definir como:

\begin{equation}
  y_i = H(w_1x_1 + w_2x_2 - \theta)
\end{equation}

O desmenuzada de la siguiente forma:

\begin{equation}
  y_i  =
  \begin{cases}
    1, si\ w_1x_1 + w_2x_2 \geq \theta\\
    0, si\ w_1x_1 + w_2x_2 \le \theta\\
  \end{cases}
\end{equation}

Si consideramos $x_1$ y $x_2$ situadas sobre el ejes de las abscisas y
ordenadas en el plano, la condición

\begin{equation}
  w_1x_1 + w_2x_2 - \theta = 0 \implies x_2 = -\frac{w_1}{w_2}x_1 + \frac{\theta}{w_2}
\end{equation}

representa una recta (hiperplano si trabajamos con n entradas), la
cuál va a dividir el plano en dos regiones, donde la neurona
proporciona una salida 0 o 1 respectivamente\cite{martin2002redes}\\

De esto se infiere que una neurona de tipo perceptrón representa un
discriminador lineal, que al implementar una condición lineal que
separa dos regiones en el espacio que representan dos tipos diferentes
de patrones.

\subsubsection{Teorema de convergencia del perceptrón}

\theoremstyle{plain}
\begin{theorem}[Teorema de convergencia del perceptrón]
Supongamos que existe algún vector de parámetros $\theta*$ tal qué
$||\theta*|| = 1$ y algún $\phi > 0$ tal que $\forall t = 1 \dots n$,
\begin{equation*}
  y_t(x_t, \theta*) \geq \phi
\end{equation*}
Además en adición, suponga que $\forall t = 1 \dots n, ||x_t|| \leq
R$\\
Entonces, el algoritmo del perceptrón tiene al menos

\begin{equation*}
  \frac{R^2}{\phi^2}
\end{equation*}

errores. La definición de error la vamos a entender como sigue: un
error ocurre cuando tenemos $y' \neq y_t$ para algún par (j, t) en el
algoritmo.
\end{theorem}

Notemos que para cualquier vector $x$, nosotros usamos $||x||$ para
referirnos a la norma Euclidiana de \textbf{x}, es decir,
$||x||=\sqrt{\displaystyle\sum_ix_i^2}$

\begin{proof}
Denotemos como $\theta^k$ para ser el vector de parámetros cuando el
algoritmo está en \textit{k-ésimo} error. Note que tenemos
\begin{equation*}
\theta_1 = 0
\end{equation*}
Ahora asumamos que el error \textit{k-ésimo} se genera cuando vemos el
ejemplar $t$, así que tenemos:
\begin{eqnarray}
  \theta^{k+1}\theta* & = & (\theta^k + y_tx_t)\theta* \label{eq:2.1}\\
                      & = & \theta^k\theta* + y_tx_t\theta* \label{eq:2.2}\\
                      & \geq & \theta^k\theta* + \phi \label{eq:2.3}
\end{eqnarray}

De la ecuación \ref{eq:2.1} sigue la definición de las actualizaciones
del perceptrón. La ecuación \ref{eq:2.3} sigue la suposición del
teorema, donde tenemos:

\begin{equation*}
  y_tx_t\theta* \geq \phi
\end{equation*}

Por inducción sobre $k$ tenemos qué:

\begin{equation*}
  \theta^{k+1}\theta* \geq k\phi
\end{equation*}

Además, como $||\theta^{k+1}|| x ||\theta*|| \geq \theta^{k+1}\theta*$
y $||\theta*|| = 1 $, tenemos que:

\begin{equation}
  \label{eq:2.4}
  ||\theta^{k+1}|| \geq k\phi
\end{equation}

Ahora como segunda parte de la demostración, vamos a tomar la cota
superior en $||\theta^{k+1}||$. Tenemos que:

\begin{eqnarray}
  ||\theta^{k+1}|| & = & ||\theta^k + y_tx_t||^2 \label{eq:2.5}\\
                   & = & ||\theta^k|| + y_t^2||x_t||^2 +
                         2y_tx_t\theta^k \label{eq:2.6}\\
                   & \leq & ||\theta^k||^2 + R^2 \label{eq:2.7}
\end{eqnarray}

La igualdad en \ref{eq:2.5} sigue la definición de actualización del
perceptrón. La ecuación \ref{eq:2.7} parte de que tenemos: 1)
$y_t^2||x_t||^2 = ||x_t||^2 \leq R^2$ por la suposición del teorema y
porqué $y_t^2=1$; 2) $y_tx_t\theta^k \leq 0$ porqué sabemos que el
vector de parámetros $\theta^k$ da error el t-ésimo ejemplo.\\
Siguiendo por inducción sobre $k$ (recordemos que $||theta^1||^2 = 0$)
qué:

\begin{equation}
  \label{eq:2.8}
  ||\theta^{k+1}||^2 \leq kR^2
\end{equation}

Combinando las cotas de las ecuaciones \ref{eq:2.4} y \ref{eq:2.8}
tehemos:

\begin{equation}
  k^2\phi^2 \leq ||\theta^{k+1}||^2 \leq kR^2
\end{equation}

de donde tenemos

\begin{equation*}
  k \leq \frac{R^2}{\phi^2}
\end{equation*}

\end{proof}



\subsection{Adalina}


\subsection{Perceptrón multicapa}


\subsection{Back Propagation}


\section{Redes Neuronales no Supervisadas}


\subsection{Mapas autoorganizados}


\subsubsection{Cuantificación optima de vectores}



\subsubsection{Modelo de neurona de Kohonen}


\subsubsection{Modelos de aprendizaje en mapas autoorganizados}



\subsection{Redes de Hopfield}



\subsection{Funciones de base radial}



\subsection{Aprendizaje vectorial cuantificado}
